Transformer는 RNN, CNN이 아닌 attention mechanism만으로 이루어진 간단한 네트워크 아키텍처이다. 이 모델은 기존 모델보다 높은 품질을 제공하면서도 병렬 처리가 가능하고 더 적은 계산량을 요구한다. 이 모델은 기계 번역 작업에서 우수함을 보여주며, 더 짧은 훈련 시간으로 기존 모델보다 높은 성능을 보인다. Transformer 모델은 다른 작업에서도 잘 작동하며, 대용량 및 한정된 데이터로 영어 구문 분석에 성공적으로 적용된다. 이번 모델은 어텐션 메커니즘만을 사용하여 모델링하며, 순환 신경망은 사용하지 않는다. 이로 인해 병렬 처리가 가능해져서 학습시간과 번역 품질에서 상당한 개선을 이룰 수 있다.
