ZeRO는 대규모 딥러닝 모델의 학습에 대한 새로운 솔루션으로, 메모리 중복성을 제거하고 훈련 속도와 모델 크기를 향상시킨다. ZeRO는 모델 병렬화를 필요로 하지 않으므로 과학자들이 적용하기 어려운 대형 모델을 학습할 수 있다. ZeRO-DP는 기존의 데이터 병렬화 및 모델 병렬화와는 다른 메모리 관리 방법으로, 계산/통신 효율을 유지하면서 메모리 효율을 달성할 수 있다. ZeRO는 최신 기술보다 8배 더 큰 모델 크기와 10배 더 높은 성능을 달성하였으며, 세계 최대 언어 모델 (17B 매개변수)을 만들었다.
ZeRO-DP는 데이터 병렬 처리에서 메모리 중복을 제거하면서 계산/통신 효율성을 유지하는 최적화 알고리즘이다. ZeRO-R은 활성화 메모리 최적화를 위해 활성화 분할을 통해 기존 MP 방법에서 활성화 복제를 인식하고 제거하는 방법을 제안하였다. ZeRO-DP와 ZeRO-R은 DL 훈련을 위한 강력한 메모리 최적화 시스템으로, ZeRO는 대규모 모델을 학습시키는 경우 MP 대신 사용하는 것이 더 효율적이다.
본 논문은 ZeRO 기술과 MP 기술의 결합으로 모델의 훈련 처리량과 속도를 증가시킬 수 있음을 입증한다. ZeRO-100B 및 ZeRO-R 알고리즘을 구현하고 평가한 결과, 현재 하드웨어의 계산 능력 내에서 1000억 개의 파라미터를 갖는 모델을 학습하는 것이 가능하다는 것을 보여준다. ZeRO-100B는 MP와 결합하여 170B 파라미터 모델을 효율적으로 실행할 수 있으며, SOTA 대비 모델 크기를 8배 이상 증가시킨다. 이를통해 대규모 모델 실험을 자유롭게 할 수 있으며, 대규모 모델 훈련의 진화와 민주화를 촉진할 것이다.
